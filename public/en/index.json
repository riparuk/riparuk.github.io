[{"content":"There are many expense tracking apps out there, but my main concern now is how to make expense tracking less manual‚Äîno need to fill in lots of data like item name, price, category, and so on.\nBut before that, there\u0026rsquo;s an experience that became the reason why expense tracking is necessary: when I was abroad. I live in Indonesia, where the cost of living is relatively cheaper compared to other countries. The currency difference made it difficult for me to compare how much I had spent so far.\nAt the time of writing this article, people often use ChatGPT for almost everything. This has become a habit because of the convenience and efficiency it provides.\nI thought, why not use a similar flow for expense tracking? So users would just need to input what they bought, the price, where they bought it, when they bought it, and so on.\nWith the current development of LLMs, I believe this is possible, and that became the foundation for why I created Traveed.\nFeatures Some of the major features of Traveed include:\nAI Expense Capture: Users only need to enter expenses by typing in everyday language, for example \u0026ldquo;Bought noodles for 15000 rupiah at Mrs. Ani\u0026rsquo;s stall\u0026rdquo;, and the AI will automatically separate that input into name, description, amount, and currency. Prompt Tuning: This is useful for customizing how the AI enters expense data. For example, if users want the AI to enter expense data in a specific format, language, or even style, they can set up prompt tuning to configure that format. 160+ Currencies Supported: Supports various currencies including IDR, USD, JPY, and others, with automatic conversion back to the main currency. This makes it easy for users to compare their expenses across different countries. Custom Categories: Users can set up their expense categories, and the AI automatically understands the available categories and uses them to categorize expenses. Summary As a developer, I use Traveed as my daily expense tracking app, and so far I\u0026rsquo;m satisfied with the existing features and continue to make improvements. It\u0026rsquo;s currently in BETA and can be accessed for free at https://traveed.app.\n","permalink":"http://localhost:55799/en/posts/traveed/","summary":"\u003cp\u003eThere are many expense tracking apps out there, but my main concern now is how to make expense tracking less manual‚Äîno need to fill in lots of data like item name, price, category, and so on.\u003c/p\u003e\n\u003cp\u003eBut before that, there\u0026rsquo;s an experience that became the reason why expense tracking is necessary: when I was abroad. I live in Indonesia, where the cost of living is relatively cheaper compared to other countries. The currency difference made it difficult for me to compare how much I had spent so far.\u003c/p\u003e","title":"Traveed"},{"content":"Meetings are indeed tiring, but they can be a way to move our work forward and straighten out what needs to be done. Here I just want to discuss from a personal POV as a technical person about when a meeting can be tiring.\nBefore that, I want to say that I like meetings, I like discussions, exchanging arguments, even debating, it\u0026rsquo;s fine for me, especially if the topic is interesting. I consider it also an opportunity for me to learn.\nThere is one thing I want to tell, so\u0026hellip; when meeting in a certain group, I\u0026rsquo;m aware if there are meeting members who seem very active in the meeting, surely that attracts attention, I believe everyone would agree with me.\nBut here, what catches my attention is the way they speak, their gestures, intonation, but wait.. here I don\u0026rsquo;t want to generalize, but there are some people I met whose way of expressing their thoughts seems forced, feels like they just want to be heard and participate in the meeting.\nThis can be seen from the way they speak, such as raising their voice, their intonation looks nervous, long-winded arguments not getting straight to the point, it\u0026rsquo;s tiring\u0026hellip;\nSometimes what makes it tiring is, because they dominate the meeting, it\u0026rsquo;s difficult for others to speak too, the topic being discussed moves to another topic because of the long-winded discussion. Even though the previous topic wasn\u0026rsquo;t clear, in the end because the technical kids are the ones doing the work, they have to think independently again after meeting.\nOne more thing, when we want to discuss a solution, indeed they provide a solution, everyone knows it is indeed a solution, but try to think, solutions for one problem can be very many, in our heads maybe it could be one solution out of many solutions being thought of, don\u0026rsquo;t just throw it into the discussion :), the meeting becomes long right. so disturbing.\nIMO or at least, we need to think first which solution is the best in our mind, it\u0026rsquo;s okay to wait for a while, and that\u0026rsquo;s what we should discuss and improve.\nI once heard from someone, people who talk excessively often try to prove themselves, this actually shows that they are insecure. I\u0026rsquo;m not saying this though.\nWe also understand, we are all learning, I\u0026rsquo;m not saying that I\u0026rsquo;m good, maybe sometimes I\u0026rsquo;m the one who talks too much, I\u0026rsquo;m also still learning, and they might also have tried hard to contribute in the meeting, without them, the meeting might feel quiet and there is no trigger, but yes, it can very well be improved together.\nI‚Äôve faced people like that at work, In my mind I feel myself wondering how to deal with these people\u0026hellip;, I think I need to be more logical than before, good in argument, I know there is something wrong in their logic in the big picture but I‚Äôm not fast enough to figure it out and explain it in a clear way. I will improve this always.\n","permalink":"http://localhost:55799/en/posts/meeting-tidak-efektif/","summary":"\u003cp\u003eMeetings are indeed tiring, but they can be a way to move our work forward and straighten out what needs to be done. Here I just want to discuss from a personal \u003cem\u003ePOV\u003c/em\u003e as a technical person about when a meeting can be tiring.\u003c/p\u003e\n\u003cp\u003eBefore that, I want to say that I like meetings, I like discussions, exchanging arguments, even debating, \u003cem\u003eit\u0026rsquo;s fine for me\u003c/em\u003e, especially if the topic is interesting. I consider it also an \u003cem\u003eopportunity\u003c/em\u003e for me to learn.\u003c/p\u003e","title":"Meetings are tiring"},{"content":"When we are working on a project, sometimes we forget some commands in the terminal, such as how to install packages, use options, or filter the use of the grep command for example.\nSometimes it‚Äôs a pain, making us need to google, look at script documentation, or ask chatGPT if we want an instant. Even though what we are looking for is only a short terminal command just because we forgot the command.\nTo make it simple, I created a script that is very easy to use, integrated with the GPT 3.5 API, and produces the on-point output we need, I named it QIAI (Quick Assistant).\nSince the latest versions, QIAI also supports Google Gemini as an alternative AI provider. This allows users to choose or switch between OpenAI and Gemini depending on preference, availability, or cost considerations.\nWhen using chatGPT, we need to adjust several things such as the OS used, the form of output we want, otherwise chatGPT will give a very, very long answer and take a long time.\nWhile using QIAI, it is more efficient, the results produced are more targeted, that‚Äôs because QIAI (for v1.1.1) has features :\nOS-Specific Answers: Provides answers specific to the operating system being used (Linux, macOS, Windows). (Windows not tested yet) Multiple AI Providers: Supports both OpenAI GPT and Google Gemini for generating terminal commands. Token Saving: Optimized prompts to reduce token usage, making it more efficient. (Avg 150 tokens per request) OpenAI GPT API is quite cheap, using the API for QIAI does not use large costs, here are the costs of gpt 3.5 that QIAI uses.\nPer 1 request, QIAI uses an average of 150 tokens, so with just $1 you can call requests 13 thousand times. quite cheap isn‚Äôt it, it doesn‚Äôt hurt if you try.\nHow to Install QIAI You can easily install it using npm globally.\nnpm install -g qiai Set the openai api key to be able to access gpt, run :\nqiai --set-openai-api-key your-api-key Alternatively, you can configure Google Gemini by setting its API key:\nqiai --set-gemini-api-key your-gemini-api-key You can switch or manage providers later without reinstalling QIAI.\nHow to Use QIAI Ask a Question, by running :\nqiai -q \u0026#34;your question\u0026#34; Thanks for reading, hope it help :)\nIf you want to collaborate, or need more features for QIAI, you can create an issue on the github repo.\nLink Github : https://github.com/riparuk/qiai\n","permalink":"http://localhost:55799/en/posts/qiai/","summary":"\u003cp\u003eWhen we are working on a project, sometimes we forget some commands in the terminal, such as how to install packages, use options, or filter the use of the grep command for example.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"QIAI Terminal Command\" loading=\"lazy\" src=\"/images/qiai/qiai-terminalcommand.webp\"\u003e\u003c/p\u003e\n\u003cp\u003eSometimes it‚Äôs a pain, making us need to google, look at script documentation, or ask chatGPT if we want an instant. Even though what we are looking for is only a short terminal command just because we forgot the command.\u003c/p\u003e","title":"QIAI (Quick Assistant)"},{"content":"I once heard a story from a close friend of mine. He was in a difficult situation, being extorted by certain parties‚ÄîI can\u0026rsquo;t mention who here haha. He told me he was threatened if he recorded what was happening.\nIt\u0026rsquo;s understandable, my friend was the victim, where could he get justice in such a situation?\nThat basis is what led me to create this software. I didn\u0026rsquo;t give it a special name, just a plain name \u0026ldquo;Hidden Cam\u0026rdquo;.\nHow to use it is simple, it can be accessed through the website here: https://hidden-cam.vercel.app/.\nDon\u0026rsquo;t worry, the video results will be stored in your own browser, not on a server. And this is also open-source, so you can see what the code looks like inside. Github: https://github.com/riparuk/hidden-cam-web.\nThe way it works is easy. Inside there is a YouTube video that can be played. The recording will run as long as the video is playing until it is stopped, that will become one recording.\nThere is an indicator in the top left as a camera preview if it is active. You can also turn off the preview if you don\u0026rsquo;t need it.\nIf you want to see your recordings, select \u0026ldquo;Saves\u0026rdquo;. You can see the recordings you have made. Recordings are saved in .webm format. I tried on iPhone Safari, unfortunately, it cannot be played, but it can still be recorded and downloaded.\nUse it wisely, Thank you :)\n","permalink":"http://localhost:55799/en/posts/hidden-cam/","summary":"\u003cp\u003eI once heard a story from a close friend of mine. He was in a difficult situation, being extorted by certain parties‚ÄîI can\u0026rsquo;t mention who here haha. He told me he was threatened if he recorded what was happening.\u003c/p\u003e\n\u003cp\u003eIt\u0026rsquo;s understandable, my friend was the victim, where could he get justice in such a situation?\u003c/p\u003e\n\u003cp\u003eThat basis is what led me to create this software. I didn\u0026rsquo;t give it a special name, just a plain name \u0026ldquo;Hidden Cam\u0026rdquo;.\u003c/p\u003e","title":"Hidden Cam Web"},{"content":"Some time ago, I was working on an e-learning related project. Simply put, teachers can create questions and students can answer them there.\nI made the system like Google Forms. After the teacher creates the questions, there will be a link that students can access to answer the questions.\nDuring the creation process, I thought, why don\u0026rsquo;t I add AI to this system? I know\u0026hellip; not all software needs AI, it depends on the needs. Here I will discuss the creation process, from ideation to implementation.\nBut here I felt that instead of the teacher having to check each student\u0026rsquo;s answer one by one, why couldn\u0026rsquo;t it happen automatically? The teacher\u0026rsquo;s POV would only focus on making questions and providing the correct answers, the rest would be handled by AI acting as a judge between the correct answer and the student\u0026rsquo;s answer.\nThis is the basis for why I created Lazy Teacher. I made it separate with the hope that it can be integrated with existing e-learning systems.\nI tried to do some research\u0026hellip; by utilizing LLM (btw this was also hype when this was written), what features could I add? Some conclusions appeared.\nLLM can act as a judge, so teachers no longer need to check student answers one by one. LLM can create questions automatically. Before I started making Lazy Teacher, I needed to do a little research. First, what kind of questions would there be, when giving grades to students usually what range do teachers give, is there tolerance if, for example, a student answers with a typo?\nFrom these questions, I created several features in Lazy Teacher.\nThere are several types of questions that can be made, I took the general ones here ShortAnswer, Essay, or MultipleChoice. Can create questions manually. Can create questions automatically, either from documents or simple prompts, and can choose the number of questions too. Can evaluate student answers automatically. The value of each question can be set. Some examples of its use.\nGenerate questions from prompt import { QuestionGenerator } from \u0026#39;lazyteacher/generator\u0026#39;; const questionGenerator = new QuestionGenerator(); const user_prompt = \u0026#34;Question about AI Agent for future\u0026#34;; const total_questions = 3; const question_type = \u0026#34;multiple_choice\u0026#34;; const generated_questions = await questionGenerator.generateFromPrompt({ question_type: question_type, user_prompt: user_prompt, total_questions: total_questions }); console.dir(generated_questions); We can set several things here, for example, the total questions, question type, and also the prompt.\nGenerate questions from document import { QuestionGenerator } from \u0026#39;lazyteacher/generator\u0026#39;; import * as fs from \u0026#34;fs\u0026#34;; const total_questions = 7; const content = fs.readFileSync(\u0026#34;example.txt\u0026#34;, \u0026#34;utf-8\u0026#34;); const questionGenerator = new QuestionGenerator(); const question_generated = await questionGenerator.generateFromDocument({ question_type: \u0026#34;short_answer\u0026#34;, document_text: content, // in string total_questions: total_questions, chunk_size: 1000, chunk_overlap: 50, document_query: \u0026#34;Jakarta\u0026#34;, // query relevant document (optional) top_query: 5 // top query relevant (optional) }); Here the concept is still the same, but there are some additional parameters such as chunk size and chunk overlap to regulate how the document is broken down, as well as document query and top query to determine which parts of the document are most relevant to take. This is actually utilizing features already provided by LangChain, and it is very helpful so that the context given to the LLM can be more controlled.\nExample output:\n[ MultipleChoice { question: \u0026#39;What is one potential benefit of AI agents mentioned in the text?\u0026#39;, correct_answer: 0, type: \u0026#39;multiple_choice\u0026#39;, choices: [ \u0026#39;Enhancing productivity\u0026#39;, \u0026#39;Increasing job displacement\u0026#39;, \u0026#39;Reducing data privacy\u0026#39;, \u0026#39;Limiting personalized experiences\u0026#39; ], answer: undefined, score: undefined }, MultipleChoice { question: \u0026#39;What is a significant challenge associated with the rise of AI agents?\u0026#39;, correct_answer: 1, type: \u0026#39;multiple_choice\u0026#39;, choices: [ \u0026#39;Streamlining operations\u0026#39;, \u0026#39;Data privacy concerns\u0026#39;, \u0026#39;Improving healthcare\u0026#39;, \u0026#39;Enhancing customer service\u0026#39; ], answer: undefined, score: undefined }, MultipleChoice { question: \u0026#39;According to the text, what is crucial as we integrate AI agents into our lives?\u0026#39;, correct_answer: 2, type: \u0026#39;multiple_choice\u0026#39;, choices: [ \u0026#39;Maximizing job displacement\u0026#39;, \u0026#39;Ignoring ethical considerations\u0026#39;, \u0026#39;Striking a balance between benefits and societal implications\u0026#39;, \u0026#39;Focusing solely on technological advancements\u0026#39; ], answer: undefined, score: undefined } ] There are still many more features that can be explored, such as related to evaluation, scores, etc. For more details, I suggest you go directly to the documentation on github, https://github.com/riparuk/LazyTeacher. Thank you for reading :)\n","permalink":"http://localhost:55799/en/posts/lazy-teacher/","summary":"\u003cp\u003eSome time ago, I was working on an e-learning related project. Simply put, teachers can create questions and students can answer them there.\u003c/p\u003e\n\u003cp\u003eI made the system like Google Forms. After the teacher creates the questions, there will be a link that students can access to answer the questions.\u003c/p\u003e\n\u003cp\u003eDuring the creation process, I thought, why don\u0026rsquo;t I add AI to this system? I know\u0026hellip; not all software needs AI, it depends on the needs. Here I will discuss the creation process, from ideation to implementation.\u003c/p\u003e","title":"Lazy Teacher"},{"content":"So, this website is my personal space that I created as an archive of knowledge and reflection‚Äîa place where I store things that I might want to reread, understand again, or share with others someday.\nIn this blog, I write in my own style to keep it simple and easy to understand. Sometimes in English, sometimes in Indonesian. The topics vary\u0026hellip; it could be IT, technical notes, tips and tricks, life philosophy, self-awareness, personal views, important lessons, and reflective things that I find relevant and useful. Not all writings are technical, and not all are reflective. I let this blog grow organically, following what I am learning and thinking about.\nThis website started on June 14, 2023. You can see the date at the very top after the title, but now I\u0026rsquo;m updating the writings here specifically to align with my goals.\nInitially, this website was built using Jekyll and hosted on GitHub Pages because it was lightweight and free.\nUpdate February 3, 2026: I now use Hugo as a static site generator, and I have moved all old writings‚Äîincluding this page‚Äîto a new repository.\nIf you want to chat, feel free to reach out to me:\n","permalink":"http://localhost:55799/en/posts/about/","summary":"\u003cp\u003eSo, this website is my personal space that I created as an \u003cstrong\u003earchive of knowledge and reflection\u003c/strong\u003e‚Äîa place where I store things that I might want to reread, understand again, or share with others someday.\u003c/p\u003e\n\u003cp\u003eIn this blog, I write in my own style to keep it simple and easy to understand. Sometimes in English, sometimes in Indonesian. The topics vary\u0026hellip; it could be IT, technical notes, tips and tricks, life philosophy, self-awareness, personal views, important lessons, and reflective things that I find relevant and useful. Not all writings are technical, and not all are reflective. I let this blog grow organically, following what I am learning and thinking about.\u003c/p\u003e","title":"What is this website?"},{"content":"Ideation Detecting tuberculosis from breath? sounds impossible right? that was when I first heard it.\nThe story started when one of my close friends invited me to join a national scale competition in Indonesia. At that time, we actually didn\u0026rsquo;t have an idea what we wanted to make yet, sounds like a trap right haha, but it\u0026rsquo;s okay, we discussed\u0026hellip; day by day we found out what potentially could win and we could make, and the idea kept changing.\nWhen we want to make an idea happen, it should make sense right (?), that\u0026rsquo;s why\u0026hellip; we read many papers, found out what others have done, what their methodology was, and so on.\nUntil finally, we made a decision to focus on the health sector and we chose to make a tuberculosis detector from breath. Sounds impossible at the time if you just heard about it, but this decision is actually possible, I will tell you why when I\u0026rsquo;m done with this post.\nAfter reading from several sources, the idea is we need breath from someone and analyze the VOCs in their breath. There are similar tools made before related to disease detection from breath, called e-nose, used for detecting COPD, diabetes, and many more. From here we understood that it is also possible if applied to tuberculosis. In several journals we also found that people infected with tuberculosis have VOCs in their breath that are different from healthy people, from there we concluded that it\u0026rsquo;s possible [1, 2].\nSimply put like that, now a problem arises. We already know what VOCs can be used to identify tuberculosis, but how can we analyze those VOCs? Maybe you adhere to using sensors (?), but you know, the tool to detect specific VOCs according to the research we got is very expensive\u0026hellip;. whereas the funds we have are impossible to buy that tool, the tool is called GC/MS (Gas Chromatography‚ÄìMass Spectrometry).\nThis is where the power of machine learning is needed. The second idea is\u0026hellip; roughly speaking we don\u0026rsquo;t need to know the specific VOCs, but we can utilize machine learning algorithms to read patterns from those VOCs. Those patterns are from breath data given to the machine learning algorithm to create a model that can identify tuberculosis from breath.\nThe concept is like our nose, when we smell durian for example, from the nose sensors we have and then the data is sent to the brain, we know it smells like durian, but we don\u0026rsquo;t know specifically about the VOCs. Therefore, we don\u0026rsquo;t need to use the GC/MS tool but just cheap gas detection sensors of the Metal Oxide Sensor type, namely MQ-2, MQ-3, MQ-4, MQ-6, MQ-7.\nPrototyping The work started from design and making the tool. This was a process that was quite long and full of trial and error, I feel it\u0026rsquo;s impossible to tell in detail, the tool can be seen as in this picture.\nThe prototype is done, now is the time for us to try the tool. The next step here is we need to collect data from patients infected with tuberculosis and healthy patients.\nHonestly, this was a process that was quite long and more tiring than making the tool. We went from one hospital to another, puskesmas (community health center), even coming directly to houses to get breath samples, certainly with health protocols according to procedure. Errors in the prototype sometimes cannot be avoided, sometimes ran out of battery, there were problems on the server, or even there were problems in the prototype itself. But here I learned a lot of things, very worth it.\nI in this team was responsible for the Software system and AI model part. What I did here was providing a place for data collection, so I created a system where the system can receive breath data from the prototype via RestFul API, and in that system also the labeling process is done which can be accessed via website.\nMachine Learning Model Data has been collected and deemed sufficient, now is the time to make the machine learning model.\nThis stage is no less tiring than the previous stage. Here I worked alone, not as a team. I had to read various articles to find out the optimal way so that the model accuracy is good from the data that has been collected previously.\nSo, each breath sample was collected for 2 minutes. There, each sensor will have its own value. The data will go through a preprocessing stage before being entered into the model, including removing outliers and filtering.\nAfter the data is cleaned, the next stage is feature selection. Based on reading articles too, here the features extracted from that data are Pearson correlation coefficients and simple descriptive statistics.\nMax Min Range Pearson Correlation Coefficient When using these features to represent information from the breath data sample as a whole (120 seconds), the resulting model accuracy was not optimal, the model could not distinguish well between TB patients and healthy individuals.\nI tried reading again some literature [3, 4], and found one using sampling techniques. Simply put, we don\u0026rsquo;t take all the data, but take the most optimal data, and the result accuracy increased. From 2 minutes of data, only 30/40 seconds at the beginning were used / the best.\nThis means, if this tool is ready to be used later, breath samples collected don\u0026rsquo;t have to wait for 2 minutes again, but just 1/4 of it.\nThese features have large data dimensions. Just imagine one breath data inside has signal data from 5 gas sensors. Before being given to the model, the data will be processed first with the Principal Component Analysis (PCA) method. This method will reduce the data dimensions to 2 dimensions and is also easy to visualize.\nThe result is like this: And the resulting model accuracy is: You can judge for yourself which algorithm is suitable for this case.\nConclusion Before closing this writing, I also want to be honest and add something. If colleagues read this writing and want to do similar innovation, there are several things that need to be paid attention to. This result needs further testing for sure. What was tested in this project focused too much on the model accuracy side only. Many other things need to be paid attention to, such as tool consistency, sensors, data collection procedures, amount of data collected, and much more. Because of limited funds and time, unfortunately, we didn\u0026rsquo;t do that optimally, but this also gives us an illustration that a project like this is very possible to do and beneficial for many people in the future.\nAchievement Projects like this, rather than doing nothing, we also entered into competitions. Alhamdulillah we got several awards. ","permalink":"http://localhost:55799/en/posts/smeltub-tuberculosis-detector/","summary":"\u003ch2 id=\"ideation\"\u003eIdeation\u003c/h2\u003e\n\u003cp\u003eDetecting tuberculosis from breath? sounds impossible right? that was when I first heard it.\u003c/p\u003e\n\u003cp\u003eThe story started when one of my close friends invited me to join a national scale competition in Indonesia. At that time, we actually didn\u0026rsquo;t have an idea what we wanted to make yet, sounds like a trap right haha, but it\u0026rsquo;s okay, we discussed\u0026hellip; day by day we found out what potentially could win and we could make, and the idea kept changing.\u003c/p\u003e","title":"Smeltub Tuberculosis Detector"},{"content":"Arch Linux is easily one of the most popular distros out there, and for good reason. The AUR is insanely convenient, the wiki reads like a full-blown manual for life, and pretty much every tweak you‚Äôll ever need is documented somewhere.\nI‚Äôve hopped distro to distro‚ÄîMint, LXDE spins, XFCE, you name it‚Äîbut I keep coming back to Arch Linux. The only thing that used to slow me down was the classic manual install. It‚Äôs powerful, sure, but wow it eats up time when all you want is a working system.\nThen I met the archinstall script. It‚Äôs the ‚ÄúI‚Äôm still cool, but also busy‚Äù way to install Arch. No shame, no gatekeeping‚Äîjust a guided flow that still lets you shape the system exactly how you like.\nArchinstall ships with the official ISO and provides a structured UI for picking disks, packages, and the rest of the setup. It doesn‚Äôt remove control; it just cuts out repetitive typing.\nBefore launching archinstall First, prep the disk you‚Äôre going to wipe. This walkthrough assumes a fresh install, so back up anything important‚Äîarchinstall will happily format the target disk. Second, grab the latest Arch ISO, flash it to a USB (Rufus, Ventoy, whatever works), boot into it, and get online. Wi-Fi via iwctl or just plug in Ethernet‚Äîdoesn‚Äôt matter, as long as you can pull packages during install. Once you‚Äôre in the live environment and connected, fire up the guided installer by typing archinstall.\nSet/Modify the below options \u0026gt; Archinstall language set: English (100%) Keyboard layout set: us Mirror region set: [] Locale language set: en_US Locale encoding set: utf-8 Drive(s) Bootloader set: systemd-bootctl Swap set: True Hostname set: archlinux Root password set: None User account Profile set: None Audio set: None Kernels set: [\u0026#39;linux\u0026#39;] Additional packages set: [] Network configuration set: Not configured, unavailable unless setup manually Timezone set: UTC Automatic time sync (NTP) set: True Optional repositories set: [] Save configuration Install (2 config(s) missing) Abort (Press \u0026#34;/\u0026#34; to search) My go-to configuration Archinstall language: English (100%) Keyboard layout: us Mirror region: leave empty or pick mirrors near you Locale language: en_US Locale encoding: utf-8 Drive(s): choose the target disk Disk layout: Wipe all (yes, this erases everything on that disk) Bootloader: systemd-bootctl or GRUB (I lean systemd-bootctl) Hostname: whatever name makes you happy‚Äîmine is often just arch Root password: I skip it and rely on sudo User account: Select ‚ÄúAdd a user‚Äù, type a username (mine‚Äôs ripa), set a password, answer ‚Äúyes‚Äù when it asks if the user should be a sudoer, then Confirm and exit Profile: pick what you need‚Äîdesktop -\u0026gt; kde if you want Plasma, and match the graphics driver to your hardware Audio: I go with pipewire Kernel: stock linux is fine unless you have a reason to change Additional packages: toss in extras like chromium if you want them right away Network configuration: Use NetworkManager Timezone: Asia/Jakarta Automatic time sync: True Optional repositories: usually just ['multilib'] so I can install Steam later When everything looks good, select Install, grab a drink while it copies files, then reboot into your fresh Arch setup. No ancient ritual required. üòÑ\n","permalink":"http://localhost:55799/en/posts/install-arch-linux-with-archinstall/","summary":"\u003cp\u003eArch Linux is easily one of the most popular distros out there, and for good reason. The \u003ca href=\"https://wiki.archlinux.org/title/Arch_User_Repository\"\u003eAUR\u003c/a\u003e is insanely convenient, the \u003ca href=\"https://wiki.archlinux.org/\"\u003ewiki\u003c/a\u003e reads like a full-blown manual for life, and pretty much every tweak you‚Äôll ever need is documented somewhere.\u003c/p\u003e\n\u003cp\u003eI‚Äôve hopped distro to distro‚ÄîMint, LXDE spins, XFCE, you name it‚Äîbut I keep coming back to \u003cstrong\u003eArch Linux\u003c/strong\u003e. The only thing that used to slow me down was the classic manual install. It‚Äôs powerful, sure, but wow it eats up time when all you want is a working system.\u003c/p\u003e","title":"Install Arch Linux with archinstall"},{"content":"Let‚Äôs get straight to it. Before doing anything fancy, make sure rclone is installed. Use whatever package manager your distro ships with‚Äîon my Arch box it‚Äôs simply sudo pacman -S rclone.\nCreate the Google Drive remote Run rclone config in your terminal. Pick n) New remote ‚Üí type n and press Enter. Give it a name. I called mine gdrive-ripa‚Äîchoose whatever makes sense for you. Select Google Drive as the storage type (in my menu it was option 18). Leave client_id and client_secret empty unless you have your own OAuth credentials. For the scope, I go with option 1: Full access to all files (drive). Skip service_account_file unless you know you need it. When asked about advanced config, I answer n) No. Say y to open a browser window for authentication, then sign in with the Google account you want to expose to rclone. I don‚Äôt use Shared Drives, so I answer n) No to that prompt too. Confirm everything with y) Yes this is OK when rclone summarizes the remote. You should now see something like this:\nCurrent remotes: Name Type ==== ==== gdrive-ripa drive e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q\u0026gt; q Hit q (or Ctrl+C) to exit the config tool.\nMount the remote Create a folder that will act like your ‚ÄúGoogle Drive‚Äù view. I use mkdir ~/Google-Drive.\nMount the remote to that folder:\nripa@arch:[~]$ rclone mount gdrive-ripa: ~/Google-Drive/ Keep that terminal window open‚Äîrclone mount runs in the foreground. Peek inside ~/Google-Drive/ and you should see the same contents as your actual Drive. If everything looks good, congrats, the mount works.\nTo unmount, just close the terminal or hit Ctrl+C in the window that‚Äôs running rclone mount.\nWant it in the background? Add \u0026amp; to the command:\nripa@arch:[~]$ rclone mount gdrive-ripa: ~/Google-Drive/ \u0026amp; That‚Äôs it‚Äînow your Google Drive behaves like any other folder on your system.\n","permalink":"http://localhost:55799/en/posts/integrating-google-drive-on-linux-with-rclone/","summary":"\u003cp\u003eLet‚Äôs get straight to it. Before doing anything fancy, make sure rclone is installed. Use whatever package manager your distro ships with‚Äîon my Arch box it‚Äôs simply \u003ccode\u003esudo pacman -S rclone\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"create-the-google-drive-remote\"\u003eCreate the Google Drive remote\u003c/h3\u003e\n\u003col\u003e\n\u003cli\u003eRun \u003ccode\u003erclone config\u003c/code\u003e in your terminal.\u003c/li\u003e\n\u003cli\u003ePick \u003ccode\u003en) New remote\u003c/code\u003e ‚Üí type \u003ccode\u003en\u003c/code\u003e and press Enter.\u003c/li\u003e\n\u003cli\u003eGive it a name. I called mine \u003ccode\u003egdrive-ripa\u003c/code\u003e‚Äîchoose whatever makes sense for you.\u003c/li\u003e\n\u003cli\u003eSelect \u003cstrong\u003eGoogle Drive\u003c/strong\u003e as the storage type (in my menu it was option 18).\u003c/li\u003e\n\u003cli\u003eLeave \u003ccode\u003eclient_id\u003c/code\u003e and \u003ccode\u003eclient_secret\u003c/code\u003e empty unless you have your own OAuth credentials.\u003c/li\u003e\n\u003cli\u003eFor the scope, I go with option 1: \u003cstrong\u003eFull access to all files (drive)\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eSkip \u003ccode\u003eservice_account_file\u003c/code\u003e unless you know you need it.\u003c/li\u003e\n\u003cli\u003eWhen asked about advanced config, I answer \u003ccode\u003en) No\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eSay \u003ccode\u003ey\u003c/code\u003e to open a browser window for authentication, then sign in with the Google account you want to expose to rclone.\u003c/li\u003e\n\u003cli\u003eI don‚Äôt use Shared Drives, so I answer \u003ccode\u003en) No\u003c/code\u003e to that prompt too.\u003c/li\u003e\n\u003cli\u003eConfirm everything with \u003ccode\u003ey) Yes this is OK\u003c/code\u003e when rclone summarizes the remote.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eYou should now see something like this:\u003c/p\u003e","title":"Integrating Google Drive on Linux with rclone"},{"content":"Here‚Äôs the quick setup flow I use whenever I need an HP printer working under Arch:\nInstall CUPS: sudo pacman -S cups Enable the CUPS service right away so it stays running after reboots: sudo systemctl enable --now cups Install HP‚Äôs tooling: sudo pacman -S hplip Plug the printer in via USB and run sudo hp-setup -i. Make sure you run it as root, otherwise you‚Äôll get the classic ‚ÄúNo device selected/specified‚Äù error. Most of the prompts can stay on their defaults‚Äîjust keep hitting Enter. Install the GUI helper: sudo pacman -S system-config-printer Launch system-config-printer, click Add, pick your printer, and choose HPLIP as the backend. Print a test page to confirm everything is talking properly. If GTK apps (Evince, Okular, etc.) complain about missing printers, install gtk3 as well. ","permalink":"http://localhost:55799/en/posts/setting-up-an-hp-printer-on-arch-linux/","summary":"\u003cp\u003eHere‚Äôs the quick setup flow I use whenever I need an HP printer working under Arch:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eInstall CUPS: \u003ccode\u003esudo pacman -S cups\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnable the CUPS service right away so it stays running after reboots: \u003ccode\u003esudo systemctl enable --now cups\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eInstall HP‚Äôs tooling: \u003ccode\u003esudo pacman -S hplip\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ePlug the printer in via USB and run \u003ccode\u003esudo hp-setup -i\u003c/code\u003e. Make sure you run it as root, otherwise you‚Äôll get the classic ‚ÄúNo device selected/specified‚Äù error. Most of the prompts can stay on their defaults‚Äîjust keep hitting Enter.\u003c/li\u003e\n\u003cli\u003eInstall the GUI helper: \u003ccode\u003esudo pacman -S system-config-printer\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eLaunch \u003cstrong\u003esystem-config-printer\u003c/strong\u003e, click \u003cstrong\u003eAdd\u003c/strong\u003e, pick your printer, and choose \u003cstrong\u003eHPLIP\u003c/strong\u003e as the backend.\u003c/li\u003e\n\u003cli\u003ePrint a test page to confirm everything is talking properly.\u003c/li\u003e\n\u003cli\u003eIf GTK apps (Evince, Okular, etc.) complain about missing printers, install \u003ccode\u003egtk3\u003c/code\u003e as well.\u003c/li\u003e\n\u003c/ol\u003e","title":"Setting Up an HP Printer on Arch Linux"},{"content":"On Linux, the whole \u0026ldquo;users and permissions\u0026rdquo; thing is one of the first concepts you have to internalize. A user is basically any account registered on the system, and permissions decide what that account can execute, read, or modify.\nHere‚Äôs the quick breakdown:\nRegular user ‚Äì the daily driver account. Limited privileges, only touches files or commands granted by the system or the user. Superuser/root ‚Äì the boss of the OS. Root can touch anything, anywhere, any time. Use it wisely. Then we have groups. Think of a group as a bundle of users that share the same permissions. Handy when you want multiple people (or processes) to access the same files or manage the same service.\nWant to create a new group? Use groupadd:\n$ groupadd webadmin Need to plug a user into that group?\n$ usermod -aG webadmin user1 Once the user is inside the group, they inherit whatever access the group has. So if a directory is locked down to webadmin, anyone in that group instantly gets the green light.\n","permalink":"http://localhost:55799/en/posts/users-groups-and-access-rights-on-linux/","summary":"\u003cp\u003eOn Linux, the whole \u0026ldquo;users and permissions\u0026rdquo; thing is one of the first concepts you have to internalize. A user is basically any account registered on the system, and permissions decide what that account can execute, read, or modify.\u003c/p\u003e\n\u003cp\u003eHere‚Äôs the quick breakdown:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eRegular user\u003c/strong\u003e ‚Äì the daily driver account. Limited privileges, only touches files or commands granted by the system or the user.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSuperuser/root\u003c/strong\u003e ‚Äì the boss of the OS. Root can touch anything, anywhere, any time. Use it wisely.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThen we have \u003cstrong\u003egroups\u003c/strong\u003e. Think of a group as a bundle of users that share the same permissions. Handy when you want multiple people (or processes) to access the same files or manage the same service.\u003c/p\u003e","title":"Users, Groups, and Access Rights on Linux"}]